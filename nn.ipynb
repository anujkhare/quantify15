{
 "metadata": {
  "name": "",
  "signature": "sha256:eaf81c5c98934e89d1ffbc8995dd08a85bd6684c7555a1caeb8a4e7dd052d85a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfpre = pd.read_csv('/root/code/gs/data/preproces.csv', header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfpre.drop([3, 4], axis=1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dftest = pd.read_csv('/root/code/gs/data/Initial_Test_Data.csv', header=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_x = numpy.array(dfpre, dtype='float32')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_y = data_x[:, -1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_x = data_x[:, :-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 247
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfpretest = pd.read_csv('/root/code/gs/data/pre_testdata.csv', header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfpretest.drop([3, 4], axis=1, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_test = numpy.array(dfpretest, dtype='int32')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import timeit\n",
      "\n",
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "\n",
      "\n",
      "from logistic_sgd import LogisticRegression\n",
      "\n",
      "\n",
      "# start-snippet-1\n",
      "class HiddenLayer(object):\n",
      "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
      "                 activation=T.tanh):\n",
      "\n",
      "        self.input = input\n",
      "        if W is None:\n",
      "            W_values = numpy.asarray(\n",
      "                rng.uniform(\n",
      "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
      "                    size=(n_in, n_out)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            if activation == theano.tensor.nnet.sigmoid:\n",
      "                W_values *= 4\n",
      "\n",
      "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
      "\n",
      "        if b is None:\n",
      "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
      "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
      "\n",
      "        self.W = W\n",
      "        self.b = b\n",
      "\n",
      "        lin_output = T.dot(input, self.W) + self.b\n",
      "        self.output = (\n",
      "            lin_output if activation is None\n",
      "            else activation(lin_output)\n",
      "        )\n",
      "        # parameters of the model\n",
      "        self.params = [self.W, self.b]\n",
      "\n",
      "\n",
      "# start-snippet-2\n",
      "class MLP(object):\n",
      "    \"\"\"Multi-Layer Perceptron Class\n",
      "\n",
      "    A multilayer perceptron is a feedforward artificial neural network model\n",
      "    that has one layer or more of hidden units and nonlinear activations.\n",
      "    Intermediate layers usually have as activation function tanh or the\n",
      "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
      "    top layer is a softmax layer (defined here by a ``LogisticRegression``\n",
      "    class).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
      "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: symbolic variable that describes the input of the\n",
      "        architecture (one minibatch)\n",
      "\n",
      "        :type n_in: int\n",
      "        :param n_in: number of input units, the dimension of the space in\n",
      "        which the datapoints lie\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden: number of hidden units\n",
      "\n",
      "        :type n_out: int\n",
      "        :param n_out: number of output units, the dimension of the space in\n",
      "        which the labels lie\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
      "        # into a HiddenLayer with a tanh activation function connected to the\n",
      "        # LogisticRegression layer; the activation function can be replaced by\n",
      "        # sigmoid or any other nonlinear function\n",
      "        self.hiddenLayer = HiddenLayer(\n",
      "            rng=rng,\n",
      "            input=input,\n",
      "            n_in=n_in,\n",
      "            n_out=n_hidden,\n",
      "            activation=T.tanh\n",
      "        )\n",
      "\n",
      "        # The logistic regression layer gets as input the hidden units\n",
      "        # of the hidden layer\n",
      "        self.logRegressionLayer = LogisticRegression(\n",
      "            input=self.hiddenLayer.output,\n",
      "            n_in=n_hidden,\n",
      "            n_out=n_out\n",
      "        )\n",
      "        # end-snippet-2 start-snippet-3\n",
      "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
      "        # be small\n",
      "        self.L1 = (\n",
      "            abs(self.hiddenLayer.W).sum()\n",
      "            + abs(self.logRegressionLayer.W).sum()\n",
      "        )\n",
      "\n",
      "        # square of L2 norm ; one regularization option is to enforce\n",
      "        # square of L2 norm to be small\n",
      "        self.L2_sqr = (\n",
      "            (self.hiddenLayer.W ** 2).sum()\n",
      "            + (self.logRegressionLayer.W ** 2).sum()\n",
      "        )\n",
      "\n",
      "        # negative log likelihood of the MLP is given by the negative\n",
      "        # log likelihood of the output of the model, computed in the\n",
      "        # logistic regression layer\n",
      "        self.negative_log_likelihood = (\n",
      "            self.logRegressionLayer.negative_log_likelihood\n",
      "        )\n",
      "        # same holds for the function computing the number of errors\n",
      "        self.errors = self.logRegressionLayer.errors\n",
      "        \n",
      "        # predictions\n",
      "        self.predictions = self.logRegressionLayer.y_pred\n",
      "        \n",
      "        # the parameters of the model are the parameters of the two layer it is\n",
      "        # made out of\n",
      "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
      "        # end-snippet-3\n",
      "\n",
      "        # keep track of model input\n",
      "        self.input = input"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data_x.shape\n",
      "# print (numpy.unique(data_y))\n",
      "print data_y.shape\n",
      "# s = 0\n",
      "# for i in numpy.unique(data_y):\n",
      "#     temp = data_y[data_y == i].shape[0]\n",
      "#     s = s + temp\n",
      "#     print temp, s\n",
      "# print s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(8287, 17)\n",
        "(8287,)\n"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = data_x\n",
      "data_x = numpy.ones((a.shape[0], a.shape[1]+1))\n",
      "data_x[:, 1:] = a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = data_test\n",
      "data_test = numpy.ones((a.shape[0], a.shape[1]+1))\n",
      "data_test[:, 1:] = a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ind = [i for i in xrange(data_x.shape[0])]\n",
      "numpy.random.shuffle(ind)\n",
      "ind_train = ind[:NT]\n",
      "ind_valid = ind[NT+1:]\n",
      "print len(ind), len(ind_train), len(ind_valid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8287 7000 1286\n"
       ]
      }
     ],
     "prompt_number": 257
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_mlp():\n",
      "    learning_rate=0.01\n",
      "    L1_reg=0.1\n",
      "    L2_reg=0.1\n",
      "    n_epochs=1000\n",
      "    batch_size=20\n",
      "    n_hidden=500\n",
      "\n",
      "    NT = 7000\n",
      "    # data_valid_x  = numpy.array()\n",
      "    train_set_x = theano.shared(numpy.asarray((data_x[ind_train, :]),\n",
      "                                                dtype = theano.config.floatX))\n",
      "    train_set_y = theano.shared(numpy.asarray((data_y[ind_train]),\n",
      "                                                dtype = 'int32'))\n",
      "\n",
      "    valid_set_x = theano.shared(numpy.asarray((data_x[ind_valid[:1280], :]),\n",
      "                                                dtype = theano.config.floatX))\n",
      "    valid_set_y = theano.shared(numpy.asarray((data_y[ind_valid[:1280]]),\n",
      "                                                dtype = 'int32'))\n",
      "    #test_set_x, test_set_y = [train_set_x, train_set_y]\n",
      "\n",
      "    # compute number of minibatches for training, validation and testing\n",
      "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "    #n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
      "\n",
      "    ######################\n",
      "    # BUILD ACTUAL MODEL #\n",
      "    ######################\n",
      "    print '... building the model'\n",
      "\n",
      "    # allocate symbolic variables for the data\n",
      "    index = T.lscalar()  # index to a [mini]batch\n",
      "    x = T.matrix('x')  # the data is presented as rasterized images\n",
      "    y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                        # [int] labels\n",
      "\n",
      "    rng = numpy.random.RandomState(1234)\n",
      "\n",
      "    # construct the MLP class\n",
      "    classifier = MLP(\n",
      "        rng=rng,\n",
      "        input=x,\n",
      "        n_in=18,\n",
      "        n_hidden=n_hidden,\n",
      "        n_out=16\n",
      "    )\n",
      "\n",
      "    # start-snippet-4\n",
      "    # the cost we minimize during training is the negative log likelihood of\n",
      "    # the model plus the regularization terms (L1 and L2); cost is expressed\n",
      "    # here symbolically\n",
      "    cost = (\n",
      "        classifier.negative_log_likelihood(y)\n",
      "        + L1_reg * classifier.L1\n",
      "        + L2_reg * classifier.L2_sqr\n",
      "    )\n",
      "    # end-snippet-4\n",
      "\n",
      "    # compiling a Theano function that computes the mistakes that are made\n",
      "    # by the model on a minibatch\n",
      "    #     #test_model = theano.function(\n",
      "    #         inputs=[index],\n",
      "    #         outputs=classifier.errors(y),\n",
      "    #         givens={\n",
      "    #             x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "    #             y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "    #         }\n",
      "    #     )\n",
      "\n",
      "    validate_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=classifier.errors(y),\n",
      "        givens={\n",
      "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
      "            y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "\n",
      "    # start-snippet-5\n",
      "    # compute the gradient of cost with respect to theta (sotred in params)\n",
      "    # the resulting gradients will be stored in a list gparams\n",
      "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
      "\n",
      "    # specify how to update the parameters of the model as a list of\n",
      "    # (variable, update expression) pairs\n",
      "\n",
      "    # given two lists of the same length, A = [a1, a2, a3, a4] and\n",
      "    # B = [b1, b2, b3, b4], zip generates a list C of same size, where each\n",
      "    # element is a pair formed from the two lists :\n",
      "    #    C = [(a1, b1), (a2, b2), (a3, b3), (a4, b4)]\n",
      "    updates = [\n",
      "        (param, param - learning_rate * gparam)\n",
      "        for param, gparam in zip(classifier.params, gparams)\n",
      "    ]\n",
      "\n",
      "    # compiling a Theano function `train_model` that returns the cost, but\n",
      "    # in the same time updates the parameter of the model based on the rules\n",
      "    # defined in `updates`\n",
      "    train_model = theano.function(\n",
      "        inputs=[index],\n",
      "        outputs=cost,\n",
      "        updates=updates,\n",
      "        givens={\n",
      "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "        }\n",
      "    )\n",
      "    # end-snippet-5\n",
      "\n",
      "    ###############\n",
      "    # TRAIN MODEL #\n",
      "    ###############\n",
      "    print '... training'\n",
      "\n",
      "    # early-stopping parameters\n",
      "    patience = 10000  # look as this many examples regardless\n",
      "    patience_increase = 2  # wait this much longer when a new best is\n",
      "                           # found\n",
      "    improvement_threshold = 0.9995  # a relative improvement of this much is\n",
      "                                   # considered significant\n",
      "    validation_frequency = min(n_train_batches, patience / 2)\n",
      "                                  # go through this many\n",
      "                                  # minibatche before checking the network\n",
      "                                  # on the validation set; in this case we\n",
      "                                  # check every epoch\n",
      "\n",
      "    best_validation_loss = numpy.inf\n",
      "    best_iter = 0\n",
      "    test_score = 0.\n",
      "    start_time = timeit.default_timer()\n",
      "\n",
      "    epoch = 0\n",
      "    done_looping = False\n",
      "\n",
      "    while (epoch < n_epochs) and (not done_looping):\n",
      "        epoch = epoch + 1\n",
      "        for minibatch_index in xrange(n_train_batches):\n",
      "\n",
      "            minibatch_avg_cost = train_model(minibatch_index)\n",
      "            # iteration number\n",
      "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
      "\n",
      "            if (iter + 1) % validation_frequency == 0:\n",
      "                # compute zero-one loss on validation set\n",
      "                validation_losses = [validate_model(i) for i\n",
      "                                     in xrange(n_valid_batches)]\n",
      "                this_validation_loss = numpy.mean(validation_losses)\n",
      "\n",
      "                print(\n",
      "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
      "                    (\n",
      "                        epoch,\n",
      "                        minibatch_index + 1,\n",
      "                        n_train_batches,\n",
      "                        this_validation_loss * 100.\n",
      "                    )\n",
      "                )\n",
      "\n",
      "                # if we got the best validation score until now\n",
      "                if this_validation_loss < best_validation_loss:\n",
      "                    #improve patience if loss improvement is good enough\n",
      "                    if (\n",
      "                        this_validation_loss < best_validation_loss *\n",
      "                        improvement_threshold\n",
      "                    ):\n",
      "                        patience = max(patience, iter * patience_increase)\n",
      "\n",
      "                    best_validation_loss = this_validation_loss\n",
      "                    best_iter = iter\n",
      "\n",
      "                    # test it on the test set\n",
      "    #                     test_losses = [test_model(i) for i\n",
      "    #                                    in xrange(n_test_batches)]\n",
      "    #                     test_score = numpy.mean(test_losses)\n",
      "\n",
      "    #                     print(('     epoch %i, minibatch %i/%i, test error of '\n",
      "    #                            'best model %f %%') %\n",
      "    #                           (epoch, minibatch_index + 1, n_train_batches,\n",
      "    #                            test_score * 100.))\n",
      "\n",
      "            if patience <= iter:\n",
      "                done_looping = True\n",
      "                break\n",
      "\n",
      "    end_time = timeit.default_timer()\n",
      "    print(('Optimization complete. Best validation score of %f %% '\n",
      "           'obtained at iteration %i, with test performance %f %%') %\n",
      "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 275
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_mlp()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "... building the model\n",
        "... training"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 1, minibatch 350/350, validation error 86.718750 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 2, minibatch 350/350, validation error 86.718750 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 3, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 4, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 5, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 6, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 7, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 8, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 9, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 10, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 11, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 12, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 13, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 14, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 15, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 16, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 17, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 18, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 19, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 20, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 21, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 22, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 23, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 24, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 25, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 26, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 27, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "epoch 28, minibatch 350/350, validation error 76.015625 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Optimization complete. Best validation score of 76.015625 % obtained at iteration 1050, with test performance 0.000000 %"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 276
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # compile a predictor function\n",
      "# predict_model = theano.function(\n",
      "#     inputs=[classifier.input],\n",
      "#     outputs=classifier.logRegressionLayer.y_pred)\n",
      "\n",
      "# test_pred = theano.function([], [self..y_pred], givens={self.x: test_set_x})\n",
      "\n",
      "test_set_x = theano.shared(numpy.asarray((data_test), dtype = theano.config.floatX))\n",
      "test_set_x = test_set_x.get_value()\n",
      "\n",
      "predict_model = theano.function(\n",
      "    inputs=[],\n",
      "    outputs=classifier.predictions,\n",
      "    givens={\n",
      "        x: test_set_x\n",
      "    }\n",
      ")\n",
      "\n",
      "predicted_values = predict_model()\n",
      "# predicted_values = predict_model(test_set_x[200:300, :])\n",
      "print predicted_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 0 0 ..., 0 0 0]\n"
       ]
      }
     ],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_values.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 262,
       "text": [
        "(4943,)"
       ]
      }
     ],
     "prompt_number": 262
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numpy.unique(predicted_values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 263,
       "text": [
        "array([0])"
       ]
      }
     ],
     "prompt_number": 263
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "with open('test.csv', 'w') as fp:\n",
      "    a = csv.writer(fp, delimiter='\\n')\n",
      "    a.writerow(predicted_values)\n",
      "#     a.writerows(predicted_values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    }
   ],
   "metadata": {}
  }
 ]
}